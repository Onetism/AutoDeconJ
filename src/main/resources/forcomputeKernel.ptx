//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-26907403
// Cuda compilation tools, release 10.1, V10.1.243
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_30
.address_size 64

	// .globl	forcomputeKernel
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry forcomputeKernel(
	.param .u64 forcomputeKernel_param_0,
	.param .u64 forcomputeKernel_param_1,
	.param .u64 forcomputeKernel_param_2,
	.param .u64 forcomputeKernel_param_3,
	.param .u32 forcomputeKernel_param_4,
	.param .f64 forcomputeKernel_param_5,
	.param .f64 forcomputeKernel_param_6,
	.param .f64 forcomputeKernel_param_7,
	.param .u32 forcomputeKernel_param_8,
	.param .u32 forcomputeKernel_param_9,
	.param .f64 forcomputeKernel_param_10,
	.param .u32 forcomputeKernel_param_11,
	.param .f64 forcomputeKernel_param_12,
	.param .f64 forcomputeKernel_param_13
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .b32 	%r<241>;
	.reg .f64 	%fd<911>;
	.reg .b64 	%rd<212>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [forcomputeKernel_param_0];
	ld.param.u64 	%rd2, [forcomputeKernel_param_1];
	ld.param.f64 	%fd236, [forcomputeKernel_param_5];
	ld.param.f64 	%fd237, [forcomputeKernel_param_6];
	ld.param.f64 	%fd238, [forcomputeKernel_param_7];
	ld.param.f64 	%fd239, [forcomputeKernel_param_10];
	ld.param.u32 	%r68, [forcomputeKernel_param_11];
	ld.param.f64 	%fd240, [forcomputeKernel_param_12];
	ld.param.f64 	%fd241, [forcomputeKernel_param_13];
	mov.u32 	%r69, %ntid.x;
	mov.u32 	%r70, %ctaid.x;
	mov.u32 	%r71, %tid.x;
	mad.lo.s32 	%r72, %r69, %r70, %r71;
	cvt.u64.u32	%rd5, %r72;
	mov.u32 	%r73, %ntid.y;
	mov.u32 	%r74, %ctaid.y;
	mov.u32 	%r75, %tid.y;
	mad.lo.s32 	%r76, %r73, %r74, %r75;
	cvt.u64.u32	%rd6, %r76;
	cvt.s64.s32	%rd7, %r68;
	add.s64 	%rd8, %rd5, %rd7;
	add.s64 	%rd9, %rd6, %rd8;
	ld.param.s32 	%rd10, [forcomputeKernel_param_4];
	setp.gt.s64	%p1, %rd9, %rd10;
	setp.gt.s64	%p2, %rd8, %rd10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB0_168;

	cvta.to.global.u64 	%rd11, %rd1;
	shl.b64 	%rd15, %rd8, 3;
	add.s64 	%rd16, %rd11, %rd15;
	cvta.to.global.u64 	%rd19, %rd2;
	shl.b64 	%rd20, %rd9, 3;
	add.s64 	%rd21, %rd19, %rd20;
	ld.global.f64 	%fd242, [%rd16];
	ld.global.f64 	%fd243, [%rd21];
	mul.f64 	%fd244, %fd243, %fd243;
	fma.rn.f64 	%fd245, %fd242, %fd242, %fd244;
	sqrt.rn.f64 	%fd1, %fd245;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd236;
	}
	and.b32  	%r1, %r85, 2147483647;
	setp.ne.s32	%p4, %r1, 2146435072;
	mov.f64 	%fd839, %fd236;
	@%p4 bra 	BB0_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r86, %temp}, %fd236;
	}
	setp.ne.s32	%p5, %r86, 0;
	mov.f64 	%fd839, %fd236;
	@%p5 bra 	BB0_4;

	mov.f64 	%fd246, 0d0000000000000000;
	mul.rn.f64 	%fd839, %fd236, %fd246;

BB0_4:
	mul.f64 	%fd247, %fd839, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r222, %fd247;
	add.u64 	%rd22, %SP, 0;
	add.u64 	%rd23, %SPL, 0;
	st.local.u32 	[%rd23], %r222;
	cvt.rn.f64.s32	%fd248, %r222;
	neg.f64 	%fd249, %fd248;
	mov.f64 	%fd250, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd251, %fd249, %fd250, %fd839;
	mov.f64 	%fd252, 0d3C91A62633145C00;
	fma.rn.f64 	%fd253, %fd249, %fd252, %fd251;
	mov.f64 	%fd254, 0d397B839A252049C0;
	fma.rn.f64 	%fd840, %fd249, %fd254, %fd253;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r87}, %fd839;
	}
	and.b32  	%r88, %r87, 2145386496;
	setp.lt.u32	%p6, %r88, 1105199104;
	@%p6 bra 	BB0_6;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd839;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd840, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r222, [%rd23];

BB0_6:
	and.b32  	%r89, %r222, 1;
	shl.b32 	%r90, %r89, 3;
	setp.eq.s32	%p7, %r89, 0;
	selp.f64	%fd255, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p7;
	mul.wide.u32 	%rd26, %r90, 8;
	mov.u64 	%rd27, __cudart_sin_cos_coeffs;
	add.s64 	%rd28, %rd26, %rd27;
	ld.const.f64 	%fd256, [%rd28+8];
	mul.rn.f64 	%fd7, %fd840, %fd840;
	fma.rn.f64 	%fd257, %fd255, %fd7, %fd256;
	ld.const.f64 	%fd258, [%rd28+16];
	fma.rn.f64 	%fd259, %fd257, %fd7, %fd258;
	ld.const.f64 	%fd260, [%rd28+24];
	fma.rn.f64 	%fd261, %fd259, %fd7, %fd260;
	ld.const.f64 	%fd262, [%rd28+32];
	fma.rn.f64 	%fd263, %fd261, %fd7, %fd262;
	ld.const.f64 	%fd264, [%rd28+40];
	fma.rn.f64 	%fd265, %fd263, %fd7, %fd264;
	ld.const.f64 	%fd266, [%rd28+48];
	fma.rn.f64 	%fd8, %fd265, %fd7, %fd266;
	fma.rn.f64 	%fd841, %fd8, %fd840, %fd840;
	@%p7 bra 	BB0_8;

	mov.f64 	%fd267, 0d3FF0000000000000;
	fma.rn.f64 	%fd841, %fd8, %fd7, %fd267;

BB0_8:
	and.b32  	%r91, %r222, 2;
	setp.eq.s32	%p8, %r91, 0;
	@%p8 bra 	BB0_10;

	mov.f64 	%fd268, 0d0000000000000000;
	mov.f64 	%fd269, 0dBFF0000000000000;
	fma.rn.f64 	%fd841, %fd841, %fd269, %fd268;

BB0_10:
	div.rn.f64 	%fd270, %fd1, %fd237;
	mul.f64 	%fd271, %fd270, %fd238;
	mul.f64 	%fd14, %fd271, %fd841;
	mul.f64 	%fd15, %fd236, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r92}, %fd15;
	}
	and.b32  	%r5, %r92, 2147483647;
	setp.ne.s32	%p9, %r5, 2146435072;
	mov.f64 	%fd843, %fd15;
	@%p9 bra 	BB0_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd15;
	}
	setp.ne.s32	%p10, %r93, 0;
	mov.f64 	%fd843, %fd15;
	@%p10 bra 	BB0_13;

	mov.f64 	%fd272, 0d0000000000000000;
	mul.rn.f64 	%fd843, %fd15, %fd272;

BB0_13:
	mul.f64 	%fd273, %fd843, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r223, %fd273;
	st.local.u32 	[%rd23], %r223;
	cvt.rn.f64.s32	%fd274, %r223;
	neg.f64 	%fd275, %fd274;
	fma.rn.f64 	%fd277, %fd275, %fd250, %fd843;
	fma.rn.f64 	%fd279, %fd275, %fd252, %fd277;
	fma.rn.f64 	%fd844, %fd275, %fd254, %fd279;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd843;
	}
	and.b32  	%r95, %r94, 2145386496;
	setp.lt.u32	%p11, %r95, 1105199104;
	@%p11 bra 	BB0_15;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd843;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd844, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r223, [%rd23];

BB0_15:
	and.b32  	%r96, %r223, 1;
	shl.b32 	%r97, %r96, 3;
	setp.eq.s32	%p12, %r96, 0;
	selp.f64	%fd281, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p12;
	mul.wide.u32 	%rd33, %r97, 8;
	add.s64 	%rd35, %rd33, %rd27;
	ld.const.f64 	%fd282, [%rd35+8];
	mul.rn.f64 	%fd21, %fd844, %fd844;
	fma.rn.f64 	%fd283, %fd281, %fd21, %fd282;
	ld.const.f64 	%fd284, [%rd35+16];
	fma.rn.f64 	%fd285, %fd283, %fd21, %fd284;
	ld.const.f64 	%fd286, [%rd35+24];
	fma.rn.f64 	%fd287, %fd285, %fd21, %fd286;
	ld.const.f64 	%fd288, [%rd35+32];
	fma.rn.f64 	%fd289, %fd287, %fd21, %fd288;
	ld.const.f64 	%fd290, [%rd35+40];
	fma.rn.f64 	%fd291, %fd289, %fd21, %fd290;
	ld.const.f64 	%fd292, [%rd35+48];
	fma.rn.f64 	%fd22, %fd291, %fd21, %fd292;
	fma.rn.f64 	%fd845, %fd22, %fd844, %fd844;
	@%p12 bra 	BB0_17;

	mov.f64 	%fd293, 0d3FF0000000000000;
	fma.rn.f64 	%fd845, %fd22, %fd21, %fd293;

BB0_17:
	and.b32  	%r98, %r223, 2;
	setp.eq.s32	%p13, %r98, 0;
	@%p13 bra 	BB0_19;

	mov.f64 	%fd294, 0d0000000000000000;
	mov.f64 	%fd295, 0dBFF0000000000000;
	fma.rn.f64 	%fd845, %fd845, %fd295, %fd294;

BB0_19:
	mov.f64 	%fd847, %fd15;
	@%p9 bra 	BB0_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r99, %temp}, %fd15;
	}
	setp.ne.s32	%p15, %r99, 0;
	mov.f64 	%fd847, %fd15;
	@%p15 bra 	BB0_22;

	mov.f64 	%fd296, 0d0000000000000000;
	mul.rn.f64 	%fd847, %fd15, %fd296;

BB0_22:
	mul.f64 	%fd297, %fd847, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r224, %fd297;
	st.local.u32 	[%rd23], %r224;
	cvt.rn.f64.s32	%fd298, %r224;
	neg.f64 	%fd299, %fd298;
	fma.rn.f64 	%fd301, %fd299, %fd250, %fd847;
	fma.rn.f64 	%fd303, %fd299, %fd252, %fd301;
	fma.rn.f64 	%fd848, %fd299, %fd254, %fd303;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r100}, %fd847;
	}
	and.b32  	%r101, %r100, 2145386496;
	setp.lt.u32	%p16, %r101, 1105199104;
	@%p16 bra 	BB0_24;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd847;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd848, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r224, [%rd23];

BB0_24:
	and.b32  	%r102, %r224, 1;
	shl.b32 	%r103, %r102, 3;
	setp.eq.s32	%p17, %r102, 0;
	selp.f64	%fd305, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p17;
	mul.wide.u32 	%rd40, %r103, 8;
	add.s64 	%rd42, %rd40, %rd27;
	ld.const.f64 	%fd306, [%rd42+8];
	mul.rn.f64 	%fd33, %fd848, %fd848;
	fma.rn.f64 	%fd307, %fd305, %fd33, %fd306;
	ld.const.f64 	%fd308, [%rd42+16];
	fma.rn.f64 	%fd309, %fd307, %fd33, %fd308;
	ld.const.f64 	%fd310, [%rd42+24];
	fma.rn.f64 	%fd311, %fd309, %fd33, %fd310;
	ld.const.f64 	%fd312, [%rd42+32];
	fma.rn.f64 	%fd313, %fd311, %fd33, %fd312;
	ld.const.f64 	%fd314, [%rd42+40];
	fma.rn.f64 	%fd315, %fd313, %fd33, %fd314;
	ld.const.f64 	%fd316, [%rd42+48];
	fma.rn.f64 	%fd34, %fd315, %fd33, %fd316;
	fma.rn.f64 	%fd849, %fd34, %fd848, %fd848;
	@%p17 bra 	BB0_26;

	mov.f64 	%fd317, 0d3FF0000000000000;
	fma.rn.f64 	%fd849, %fd34, %fd33, %fd317;

BB0_26:
	and.b32  	%r104, %r224, 2;
	setp.eq.s32	%p18, %r104, 0;
	@%p18 bra 	BB0_28;

	mov.f64 	%fd318, 0d0000000000000000;
	mov.f64 	%fd319, 0dBFF0000000000000;
	fma.rn.f64 	%fd849, %fd849, %fd319, %fd318;

BB0_28:
	mul.f64 	%fd320, %fd845, %fd849;
	mul.f64 	%fd321, %fd238, 0d4010000000000000;
	mul.f64 	%fd322, %fd321, %fd239;
	mul.f64 	%fd323, %fd322, %fd320;
	mul.f64 	%fd324, %fd323, 0d426D1A94A2000000;
	cvt.rzi.s64.f64	%rd43, %fd324;
	cvt.rn.f64.s64	%fd325, %rd43;
	mul.f64 	%fd40, %fd325, 0dBD719799812DEA11;
	mov.f64 	%fd851, %fd15;
	@%p9 bra 	BB0_31;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r105, %temp}, %fd15;
	}
	setp.ne.s32	%p20, %r105, 0;
	mov.f64 	%fd851, %fd15;
	@%p20 bra 	BB0_31;

	mov.f64 	%fd326, 0d0000000000000000;
	mul.rn.f64 	%fd851, %fd15, %fd326;

BB0_31:
	mul.f64 	%fd327, %fd851, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r225, %fd327;
	st.local.u32 	[%rd23], %r225;
	cvt.rn.f64.s32	%fd328, %r225;
	neg.f64 	%fd329, %fd328;
	fma.rn.f64 	%fd331, %fd329, %fd250, %fd851;
	fma.rn.f64 	%fd333, %fd329, %fd252, %fd331;
	fma.rn.f64 	%fd852, %fd329, %fd254, %fd333;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd851;
	}
	and.b32  	%r107, %r106, 2145386496;
	setp.lt.u32	%p21, %r107, 1105199104;
	@%p21 bra 	BB0_33;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd851;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd852, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r225, [%rd23];

BB0_33:
	and.b32  	%r108, %r225, 1;
	shl.b32 	%r109, %r108, 3;
	setp.eq.s32	%p22, %r108, 0;
	selp.f64	%fd335, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p22;
	mul.wide.u32 	%rd48, %r109, 8;
	add.s64 	%rd50, %rd48, %rd27;
	ld.const.f64 	%fd336, [%rd50+8];
	mul.rn.f64 	%fd46, %fd852, %fd852;
	fma.rn.f64 	%fd337, %fd335, %fd46, %fd336;
	ld.const.f64 	%fd338, [%rd50+16];
	fma.rn.f64 	%fd339, %fd337, %fd46, %fd338;
	ld.const.f64 	%fd340, [%rd50+24];
	fma.rn.f64 	%fd341, %fd339, %fd46, %fd340;
	ld.const.f64 	%fd342, [%rd50+32];
	fma.rn.f64 	%fd343, %fd341, %fd46, %fd342;
	ld.const.f64 	%fd344, [%rd50+40];
	fma.rn.f64 	%fd345, %fd343, %fd46, %fd344;
	ld.const.f64 	%fd346, [%rd50+48];
	fma.rn.f64 	%fd47, %fd345, %fd46, %fd346;
	fma.rn.f64 	%fd853, %fd47, %fd852, %fd852;
	@%p22 bra 	BB0_35;

	mov.f64 	%fd347, 0d3FF0000000000000;
	fma.rn.f64 	%fd853, %fd47, %fd46, %fd347;

BB0_35:
	and.b32  	%r110, %r225, 2;
	setp.eq.s32	%p23, %r110, 0;
	@%p23 bra 	BB0_37;

	mov.f64 	%fd348, 0d0000000000000000;
	mov.f64 	%fd349, 0dBFF0000000000000;
	fma.rn.f64 	%fd853, %fd853, %fd349, %fd348;

BB0_37:
	mov.f64 	%fd855, %fd15;
	@%p9 bra 	BB0_40;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r111, %temp}, %fd15;
	}
	setp.ne.s32	%p25, %r111, 0;
	mov.f64 	%fd855, %fd15;
	@%p25 bra 	BB0_40;

	mov.f64 	%fd350, 0d0000000000000000;
	mul.rn.f64 	%fd855, %fd15, %fd350;

BB0_40:
	mul.f64 	%fd351, %fd855, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r226, %fd351;
	st.local.u32 	[%rd23], %r226;
	cvt.rn.f64.s32	%fd352, %r226;
	neg.f64 	%fd353, %fd352;
	fma.rn.f64 	%fd355, %fd353, %fd250, %fd855;
	fma.rn.f64 	%fd357, %fd353, %fd252, %fd355;
	fma.rn.f64 	%fd856, %fd353, %fd254, %fd357;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r112}, %fd855;
	}
	and.b32  	%r113, %r112, 2145386496;
	setp.lt.u32	%p26, %r113, 1105199104;
	@%p26 bra 	BB0_42;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd855;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd856, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r226, [%rd23];

BB0_42:
	and.b32  	%r114, %r226, 1;
	shl.b32 	%r115, %r114, 3;
	setp.eq.s32	%p27, %r114, 0;
	selp.f64	%fd359, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p27;
	mul.wide.u32 	%rd55, %r115, 8;
	add.s64 	%rd57, %rd55, %rd27;
	ld.const.f64 	%fd360, [%rd57+8];
	mul.rn.f64 	%fd58, %fd856, %fd856;
	fma.rn.f64 	%fd361, %fd359, %fd58, %fd360;
	ld.const.f64 	%fd362, [%rd57+16];
	fma.rn.f64 	%fd363, %fd361, %fd58, %fd362;
	ld.const.f64 	%fd364, [%rd57+24];
	fma.rn.f64 	%fd365, %fd363, %fd58, %fd364;
	ld.const.f64 	%fd366, [%rd57+32];
	fma.rn.f64 	%fd367, %fd365, %fd58, %fd366;
	ld.const.f64 	%fd368, [%rd57+40];
	fma.rn.f64 	%fd369, %fd367, %fd58, %fd368;
	ld.const.f64 	%fd370, [%rd57+48];
	fma.rn.f64 	%fd59, %fd369, %fd58, %fd370;
	fma.rn.f64 	%fd857, %fd59, %fd856, %fd856;
	@%p27 bra 	BB0_44;

	mov.f64 	%fd371, 0d3FF0000000000000;
	fma.rn.f64 	%fd857, %fd59, %fd58, %fd371;

BB0_44:
	and.b32  	%r116, %r226, 2;
	setp.eq.s32	%p28, %r116, 0;
	@%p28 bra 	BB0_46;

	mov.f64 	%fd372, 0d0000000000000000;
	mov.f64 	%fd373, 0dBFF0000000000000;
	fma.rn.f64 	%fd857, %fd857, %fd373, %fd372;

BB0_46:
	mul.f64 	%fd376, %fd853, %fd857;
	mul.f64 	%fd377, %fd376, 0d4010000000000000;
	div.rn.f64 	%fd65, %fd40, %fd377;
	div.rn.f64 	%fd66, %fd236, 0d40C4000000000000;
	mul.f64 	%fd378, %fd240, %fd241;
	mul.f64 	%fd379, %fd378, %fd240;
	mul.f64 	%fd380, %fd379, %fd241;
	div.rn.f64 	%fd67, %fd237, %fd380;
	mov.f64 	%fd859, 0d0000000000000000;
	mov.u32 	%r227, 0;
	mov.f64 	%fd860, %fd859;

BB0_47:
	cvt.rn.f64.s32	%fd381, %r227;
	mul.f64 	%fd899, %fd66, %fd381;
	mul.f64 	%fd865, %fd899, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r118}, %fd865;
	}
	and.b32  	%r19, %r118, 2147483647;
	setp.ne.s32	%p29, %r19, 2146435072;
	mov.f64 	%fd861, %fd865;
	@%p29 bra 	BB0_50;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r119, %temp}, %fd865;
	}
	setp.ne.s32	%p30, %r119, 0;
	mov.f64 	%fd861, %fd865;
	@%p30 bra 	BB0_50;

	mov.f64 	%fd382, 0d0000000000000000;
	mul.rn.f64 	%fd861, %fd865, %fd382;

BB0_50:
	mul.f64 	%fd383, %fd861, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r228, %fd383;
	st.local.u32 	[%rd23], %r228;
	cvt.rn.f64.s32	%fd384, %r228;
	neg.f64 	%fd385, %fd384;
	fma.rn.f64 	%fd387, %fd385, %fd250, %fd861;
	fma.rn.f64 	%fd389, %fd385, %fd252, %fd387;
	fma.rn.f64 	%fd862, %fd385, %fd254, %fd389;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r120}, %fd861;
	}
	and.b32  	%r121, %r120, 2145386496;
	setp.lt.u32	%p31, %r121, 1105199104;
	@%p31 bra 	BB0_52;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd861;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd862, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r228, [%rd23];

BB0_52:
	and.b32  	%r122, %r228, 1;
	shl.b32 	%r123, %r122, 3;
	setp.eq.s32	%p32, %r122, 0;
	selp.f64	%fd391, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p32;
	mul.wide.u32 	%rd62, %r123, 8;
	add.s64 	%rd64, %rd62, %rd27;
	ld.const.f64 	%fd392, [%rd64+8];
	mul.rn.f64 	%fd77, %fd862, %fd862;
	fma.rn.f64 	%fd393, %fd391, %fd77, %fd392;
	ld.const.f64 	%fd394, [%rd64+16];
	fma.rn.f64 	%fd395, %fd393, %fd77, %fd394;
	ld.const.f64 	%fd396, [%rd64+24];
	fma.rn.f64 	%fd397, %fd395, %fd77, %fd396;
	ld.const.f64 	%fd398, [%rd64+32];
	fma.rn.f64 	%fd399, %fd397, %fd77, %fd398;
	ld.const.f64 	%fd400, [%rd64+40];
	fma.rn.f64 	%fd401, %fd399, %fd77, %fd400;
	ld.const.f64 	%fd402, [%rd64+48];
	fma.rn.f64 	%fd78, %fd401, %fd77, %fd402;
	fma.rn.f64 	%fd863, %fd78, %fd862, %fd862;
	@%p32 bra 	BB0_54;

	mov.f64 	%fd403, 0d3FF0000000000000;
	fma.rn.f64 	%fd863, %fd78, %fd77, %fd403;

BB0_54:
	and.b32  	%r124, %r228, 2;
	setp.eq.s32	%p33, %r124, 0;
	@%p33 bra 	BB0_56;

	mov.f64 	%fd404, 0d0000000000000000;
	mov.f64 	%fd405, 0dBFF0000000000000;
	fma.rn.f64 	%fd863, %fd863, %fd405, %fd404;

BB0_56:
	@%p29 bra 	BB0_59;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r125, %temp}, %fd865;
	}
	setp.ne.s32	%p35, %r125, 0;
	@%p35 bra 	BB0_59;

	mov.f64 	%fd406, 0d0000000000000000;
	mul.rn.f64 	%fd865, %fd865, %fd406;

BB0_59:
	mul.f64 	%fd407, %fd865, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r229, %fd407;
	st.local.u32 	[%rd23], %r229;
	cvt.rn.f64.s32	%fd408, %r229;
	neg.f64 	%fd409, %fd408;
	fma.rn.f64 	%fd411, %fd409, %fd250, %fd865;
	fma.rn.f64 	%fd413, %fd409, %fd252, %fd411;
	fma.rn.f64 	%fd866, %fd409, %fd254, %fd413;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r126}, %fd865;
	}
	and.b32  	%r127, %r126, 2145386496;
	setp.lt.u32	%p36, %r127, 1105199104;
	@%p36 bra 	BB0_61;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd865;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd866, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r229, [%rd23];

BB0_61:
	and.b32  	%r128, %r229, 1;
	shl.b32 	%r129, %r128, 3;
	setp.eq.s32	%p37, %r128, 0;
	selp.f64	%fd415, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p37;
	mul.wide.u32 	%rd69, %r129, 8;
	add.s64 	%rd71, %rd69, %rd27;
	ld.const.f64 	%fd416, [%rd71+8];
	mul.rn.f64 	%fd89, %fd866, %fd866;
	fma.rn.f64 	%fd417, %fd415, %fd89, %fd416;
	ld.const.f64 	%fd418, [%rd71+16];
	fma.rn.f64 	%fd419, %fd417, %fd89, %fd418;
	ld.const.f64 	%fd420, [%rd71+24];
	fma.rn.f64 	%fd421, %fd419, %fd89, %fd420;
	ld.const.f64 	%fd422, [%rd71+32];
	fma.rn.f64 	%fd423, %fd421, %fd89, %fd422;
	ld.const.f64 	%fd424, [%rd71+40];
	fma.rn.f64 	%fd425, %fd423, %fd89, %fd424;
	ld.const.f64 	%fd426, [%rd71+48];
	fma.rn.f64 	%fd90, %fd425, %fd89, %fd426;
	fma.rn.f64 	%fd867, %fd90, %fd866, %fd866;
	@%p37 bra 	BB0_63;

	mov.f64 	%fd427, 0d3FF0000000000000;
	fma.rn.f64 	%fd867, %fd90, %fd89, %fd427;

BB0_63:
	and.b32  	%r130, %r229, 2;
	setp.eq.s32	%p38, %r130, 0;
	@%p38 bra 	BB0_65;

	mov.f64 	%fd428, 0d0000000000000000;
	mov.f64 	%fd429, 0dBFF0000000000000;
	fma.rn.f64 	%fd867, %fd867, %fd429, %fd428;

BB0_65:
	mul.f64 	%fd96, %fd863, %fd867;
	mov.f64 	%fd869, %fd15;
	@%p9 bra 	BB0_68;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r131, %temp}, %fd15;
	}
	setp.ne.s32	%p40, %r131, 0;
	mov.f64 	%fd869, %fd15;
	@%p40 bra 	BB0_68;

	mov.f64 	%fd430, 0d0000000000000000;
	mul.rn.f64 	%fd869, %fd15, %fd430;

BB0_68:
	mul.f64 	%fd431, %fd869, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r230, %fd431;
	st.local.u32 	[%rd23], %r230;
	cvt.rn.f64.s32	%fd432, %r230;
	neg.f64 	%fd433, %fd432;
	fma.rn.f64 	%fd435, %fd433, %fd250, %fd869;
	fma.rn.f64 	%fd437, %fd433, %fd252, %fd435;
	fma.rn.f64 	%fd870, %fd433, %fd254, %fd437;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r132}, %fd869;
	}
	and.b32  	%r133, %r132, 2145386496;
	setp.lt.u32	%p41, %r133, 1105199104;
	@%p41 bra 	BB0_70;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd869;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd870, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r230, [%rd23];

BB0_70:
	and.b32  	%r134, %r230, 1;
	shl.b32 	%r135, %r134, 3;
	setp.eq.s32	%p42, %r134, 0;
	selp.f64	%fd439, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p42;
	mul.wide.u32 	%rd76, %r135, 8;
	add.s64 	%rd78, %rd76, %rd27;
	ld.const.f64 	%fd440, [%rd78+8];
	mul.rn.f64 	%fd102, %fd870, %fd870;
	fma.rn.f64 	%fd441, %fd439, %fd102, %fd440;
	ld.const.f64 	%fd442, [%rd78+16];
	fma.rn.f64 	%fd443, %fd441, %fd102, %fd442;
	ld.const.f64 	%fd444, [%rd78+24];
	fma.rn.f64 	%fd445, %fd443, %fd102, %fd444;
	ld.const.f64 	%fd446, [%rd78+32];
	fma.rn.f64 	%fd447, %fd445, %fd102, %fd446;
	ld.const.f64 	%fd448, [%rd78+40];
	fma.rn.f64 	%fd449, %fd447, %fd102, %fd448;
	ld.const.f64 	%fd450, [%rd78+48];
	fma.rn.f64 	%fd103, %fd449, %fd102, %fd450;
	fma.rn.f64 	%fd871, %fd103, %fd870, %fd870;
	@%p42 bra 	BB0_72;

	mov.f64 	%fd451, 0d3FF0000000000000;
	fma.rn.f64 	%fd871, %fd103, %fd102, %fd451;

BB0_72:
	and.b32  	%r136, %r230, 2;
	setp.eq.s32	%p43, %r136, 0;
	@%p43 bra 	BB0_74;

	mov.f64 	%fd452, 0d0000000000000000;
	mov.f64 	%fd453, 0dBFF0000000000000;
	fma.rn.f64 	%fd871, %fd871, %fd453, %fd452;

BB0_74:
	mov.f64 	%fd873, %fd15;
	@%p9 bra 	BB0_77;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r137, %temp}, %fd15;
	}
	setp.ne.s32	%p45, %r137, 0;
	mov.f64 	%fd873, %fd15;
	@%p45 bra 	BB0_77;

	mov.f64 	%fd454, 0d0000000000000000;
	mul.rn.f64 	%fd873, %fd15, %fd454;

BB0_77:
	mul.f64 	%fd455, %fd873, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r231, %fd455;
	st.local.u32 	[%rd23], %r231;
	cvt.rn.f64.s32	%fd456, %r231;
	neg.f64 	%fd457, %fd456;
	fma.rn.f64 	%fd459, %fd457, %fd250, %fd873;
	fma.rn.f64 	%fd461, %fd457, %fd252, %fd459;
	fma.rn.f64 	%fd874, %fd457, %fd254, %fd461;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r138}, %fd873;
	}
	and.b32  	%r139, %r138, 2145386496;
	setp.lt.u32	%p46, %r139, 1105199104;
	@%p46 bra 	BB0_79;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd873;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd874, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r231, [%rd23];

BB0_79:
	and.b32  	%r140, %r231, 1;
	shl.b32 	%r141, %r140, 3;
	setp.eq.s32	%p47, %r140, 0;
	selp.f64	%fd463, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p47;
	mul.wide.u32 	%rd83, %r141, 8;
	add.s64 	%rd85, %rd83, %rd27;
	ld.const.f64 	%fd464, [%rd85+8];
	mul.rn.f64 	%fd114, %fd874, %fd874;
	fma.rn.f64 	%fd465, %fd463, %fd114, %fd464;
	ld.const.f64 	%fd466, [%rd85+16];
	fma.rn.f64 	%fd467, %fd465, %fd114, %fd466;
	ld.const.f64 	%fd468, [%rd85+24];
	fma.rn.f64 	%fd469, %fd467, %fd114, %fd468;
	ld.const.f64 	%fd470, [%rd85+32];
	fma.rn.f64 	%fd471, %fd469, %fd114, %fd470;
	ld.const.f64 	%fd472, [%rd85+40];
	fma.rn.f64 	%fd473, %fd471, %fd114, %fd472;
	ld.const.f64 	%fd474, [%rd85+48];
	fma.rn.f64 	%fd115, %fd473, %fd114, %fd474;
	fma.rn.f64 	%fd875, %fd115, %fd874, %fd874;
	@%p47 bra 	BB0_81;

	mov.f64 	%fd475, 0d3FF0000000000000;
	fma.rn.f64 	%fd875, %fd115, %fd114, %fd475;

BB0_81:
	and.b32  	%r142, %r231, 2;
	setp.eq.s32	%p48, %r142, 0;
	@%p48 bra 	BB0_83;

	mov.f64 	%fd476, 0d0000000000000000;
	mov.f64 	%fd477, 0dBFF0000000000000;
	fma.rn.f64 	%fd875, %fd875, %fd477, %fd476;

BB0_83:
	mul.f64 	%fd478, %fd871, %fd875;
	fma.rn.f64 	%fd479, %fd871, %fd875, %fd478;
	mul.f64 	%fd480, %fd40, %fd96;
	div.rn.f64 	%fd121, %fd480, %fd479;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r143}, %fd899;
	}
	and.b32  	%r32, %r143, 2147483647;
	setp.ne.s32	%p49, %r32, 2146435072;
	mov.f64 	%fd877, %fd899;
	@%p49 bra 	BB0_86;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r144, %temp}, %fd899;
	}
	setp.ne.s32	%p50, %r144, 0;
	mov.f64 	%fd877, %fd899;
	@%p50 bra 	BB0_86;

	mov.f64 	%fd481, 0d0000000000000000;
	mul.rn.f64 	%fd877, %fd899, %fd481;

BB0_86:
	mul.f64 	%fd482, %fd877, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r232, %fd482;
	st.local.u32 	[%rd23], %r232;
	cvt.rn.f64.s32	%fd483, %r232;
	neg.f64 	%fd484, %fd483;
	fma.rn.f64 	%fd486, %fd484, %fd250, %fd877;
	fma.rn.f64 	%fd488, %fd484, %fd252, %fd486;
	fma.rn.f64 	%fd878, %fd484, %fd254, %fd488;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r145}, %fd877;
	}
	and.b32  	%r146, %r145, 2145386496;
	setp.lt.u32	%p51, %r146, 1105199104;
	@%p51 bra 	BB0_88;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd877;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd878, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r232, [%rd23];

BB0_88:
	and.b32  	%r147, %r232, 1;
	shl.b32 	%r148, %r147, 3;
	setp.eq.s32	%p52, %r147, 0;
	selp.f64	%fd490, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p52;
	mul.wide.u32 	%rd90, %r148, 8;
	add.s64 	%rd92, %rd90, %rd27;
	ld.const.f64 	%fd491, [%rd92+8];
	mul.rn.f64 	%fd127, %fd878, %fd878;
	fma.rn.f64 	%fd492, %fd490, %fd127, %fd491;
	ld.const.f64 	%fd493, [%rd92+16];
	fma.rn.f64 	%fd494, %fd492, %fd127, %fd493;
	ld.const.f64 	%fd495, [%rd92+24];
	fma.rn.f64 	%fd496, %fd494, %fd127, %fd495;
	ld.const.f64 	%fd497, [%rd92+32];
	fma.rn.f64 	%fd498, %fd496, %fd127, %fd497;
	ld.const.f64 	%fd499, [%rd92+40];
	fma.rn.f64 	%fd500, %fd498, %fd127, %fd499;
	ld.const.f64 	%fd501, [%rd92+48];
	fma.rn.f64 	%fd128, %fd500, %fd127, %fd501;
	fma.rn.f64 	%fd879, %fd128, %fd878, %fd878;
	@%p52 bra 	BB0_90;

	mov.f64 	%fd502, 0d3FF0000000000000;
	fma.rn.f64 	%fd879, %fd128, %fd127, %fd502;

BB0_90:
	and.b32  	%r149, %r232, 2;
	setp.eq.s32	%p53, %r149, 0;
	@%p53 bra 	BB0_92;

	mov.f64 	%fd503, 0d0000000000000000;
	mov.f64 	%fd504, 0dBFF0000000000000;
	fma.rn.f64 	%fd879, %fd879, %fd504, %fd503;

BB0_92:
	mov.f64 	%fd881, %fd236;
	@%p4 bra 	BB0_95;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r150, %temp}, %fd236;
	}
	setp.ne.s32	%p55, %r150, 0;
	mov.f64 	%fd881, %fd236;
	@%p55 bra 	BB0_95;

	mov.f64 	%fd505, 0d0000000000000000;
	mul.rn.f64 	%fd881, %fd236, %fd505;

BB0_95:
	mul.f64 	%fd506, %fd881, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r233, %fd506;
	st.local.u32 	[%rd23], %r233;
	cvt.rn.f64.s32	%fd507, %r233;
	neg.f64 	%fd508, %fd507;
	fma.rn.f64 	%fd510, %fd508, %fd250, %fd881;
	fma.rn.f64 	%fd512, %fd508, %fd252, %fd510;
	fma.rn.f64 	%fd882, %fd508, %fd254, %fd512;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r151}, %fd881;
	}
	and.b32  	%r152, %r151, 2145386496;
	setp.lt.u32	%p56, %r152, 1105199104;
	@%p56 bra 	BB0_97;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd881;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd882, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r233, [%rd23];

BB0_97:
	and.b32  	%r153, %r233, 1;
	shl.b32 	%r154, %r153, 3;
	setp.eq.s32	%p57, %r153, 0;
	selp.f64	%fd514, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p57;
	mul.wide.u32 	%rd97, %r154, 8;
	add.s64 	%rd99, %rd97, %rd27;
	ld.const.f64 	%fd515, [%rd99+8];
	mul.rn.f64 	%fd139, %fd882, %fd882;
	fma.rn.f64 	%fd516, %fd514, %fd139, %fd515;
	ld.const.f64 	%fd517, [%rd99+16];
	fma.rn.f64 	%fd518, %fd516, %fd139, %fd517;
	ld.const.f64 	%fd519, [%rd99+24];
	fma.rn.f64 	%fd520, %fd518, %fd139, %fd519;
	ld.const.f64 	%fd521, [%rd99+32];
	fma.rn.f64 	%fd522, %fd520, %fd139, %fd521;
	ld.const.f64 	%fd523, [%rd99+40];
	fma.rn.f64 	%fd524, %fd522, %fd139, %fd523;
	ld.const.f64 	%fd525, [%rd99+48];
	fma.rn.f64 	%fd140, %fd524, %fd139, %fd525;
	fma.rn.f64 	%fd883, %fd140, %fd882, %fd882;
	@%p57 bra 	BB0_99;

	mov.f64 	%fd526, 0d3FF0000000000000;
	fma.rn.f64 	%fd883, %fd140, %fd139, %fd526;

BB0_99:
	and.b32  	%r155, %r233, 2;
	setp.eq.s32	%p58, %r155, 0;
	@%p58 bra 	BB0_101;

	mov.f64 	%fd527, 0d0000000000000000;
	mov.f64 	%fd528, 0dBFF0000000000000;
	fma.rn.f64 	%fd883, %fd883, %fd528, %fd527;

BB0_101:
	mul.f64 	%fd529, %fd14, %fd879;
	div.rn.f64 	%fd530, %fd529, %fd883;
	setp.lt.f64	%p59, %fd530, 0d0000000000000000;
	neg.f64 	%fd531, %fd530;
	selp.f64	%fd532, %fd531, %fd530, %p59;
	abs.f64 	%fd146, %fd532;
	setp.gtu.f64	%p60, %fd146, 0d400FB319F277BBE5;
	@%p60 bra 	BB0_103;
	bra.uni 	BB0_102;

BB0_103:
	setp.gtu.f64	%p61, %fd146, 0d401C58FD1A62F5EC;
	@%p61 bra 	BB0_105;
	bra.uni 	BB0_104;

BB0_105:
	setp.gtu.f64	%p62, %fd146, 0d402471FCB6A7A8C0;
	@%p62 bra 	BB0_107;
	bra.uni 	BB0_106;

BB0_107:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r156}, %fd146;
	}
	and.b32  	%r157, %r156, 2147483647;
	setp.ne.s32	%p63, %r157, 2146435072;
	@%p63 bra 	BB0_109;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r158, %temp}, %fd146;
	}
	setp.eq.s32	%p64, %r158, 0;
	mov.f64 	%fd890, 0d0000000000000000;
	@%p64 bra 	BB0_121;

BB0_109:
	neg.f64 	%fd639, %fd146;
	rcp.approx.ftz.f64 	%fd640, %fd146;
	mov.f64 	%fd641, 0d3FF0000000000000;
	fma.rn.f64 	%fd642, %fd639, %fd640, %fd641;
	fma.rn.f64 	%fd643, %fd642, %fd642, %fd642;
	fma.rn.f64 	%fd644, %fd643, %fd640, %fd640;
	mul.f64 	%fd645, %fd644, %fd644;
	mov.f64 	%fd646, 0d409927467A655012;
	mov.f64 	%fd647, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd648, %fd647, %fd645, %fd646;
	mov.f64 	%fd649, 0dC05751787E247BD4;
	fma.rn.f64 	%fd650, %fd648, %fd645, %fd649;
	mov.f64 	%fd651, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd652, %fd650, %fd645, %fd651;
	mov.f64 	%fd653, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd654, %fd652, %fd645, %fd653;
	mov.f64 	%fd655, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd656, %fd654, %fd645, %fd655;
	mov.f64 	%fd657, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd658, %fd656, %fd645, %fd657;
	mov.f64 	%fd659, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd660, %fd658, %fd645, %fd659;
	mov.f64 	%fd661, 0d410ECD4523B12B84;
	mov.f64 	%fd662, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd663, %fd662, %fd645, %fd661;
	mov.f64 	%fd664, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd665, %fd663, %fd645, %fd664;
	mov.f64 	%fd666, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd667, %fd665, %fd645, %fd666;
	mov.f64 	%fd668, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd669, %fd667, %fd645, %fd668;
	mov.f64 	%fd670, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd671, %fd669, %fd645, %fd670;
	mov.f64 	%fd672, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd673, %fd671, %fd645, %fd672;
	mov.f64 	%fd674, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd675, %fd673, %fd645, %fd674;
	mov.f64 	%fd676, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd677, %fd675, %fd645, %fd676;
	fma.rn.f64 	%fd150, %fd677, %fd644, %fd146;
	rsqrt.approx.f64 	%fd678, %fd146;
	mul.f64 	%fd679, %fd678, 0d3FE9884533D43651;
	mul.f64 	%fd151, %fd660, %fd679;
	mul.f64 	%fd680, %fd150, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r234, %fd680;
	st.local.u32 	[%rd23], %r234;
	cvt.rn.f64.s32	%fd681, %r234;
	neg.f64 	%fd682, %fd681;
	fma.rn.f64 	%fd684, %fd682, %fd250, %fd150;
	fma.rn.f64 	%fd686, %fd682, %fd252, %fd684;
	fma.rn.f64 	%fd885, %fd682, %fd254, %fd686;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r159}, %fd150;
	}
	and.b32  	%r160, %r159, 2145386496;
	setp.lt.u32	%p65, %r160, 1105199104;
	@%p65 bra 	BB0_111;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd150;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd885, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r234, [%rd23];

BB0_111:
	and.b32  	%r161, %r234, 3;
	cvt.rn.f64.s32	%fd688, %r161;
	add.f64 	%fd689, %fd885, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd886, %fd688, 0d3FF921FB54442D18, %fd689;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r162}, %fd886;
	}
	and.b32  	%r163, %r162, 2147483647;
	setp.ne.s32	%p66, %r163, 2146435072;
	@%p66 bra 	BB0_114;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r164, %temp}, %fd886;
	}
	setp.ne.s32	%p67, %r164, 0;
	@%p67 bra 	BB0_114;

	mov.f64 	%fd690, 0d0000000000000000;
	mul.rn.f64 	%fd886, %fd886, %fd690;

BB0_114:
	mul.f64 	%fd691, %fd886, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r235, %fd691;
	st.local.u32 	[%rd23], %r235;
	cvt.rn.f64.s32	%fd692, %r235;
	neg.f64 	%fd693, %fd692;
	fma.rn.f64 	%fd695, %fd693, %fd250, %fd886;
	fma.rn.f64 	%fd697, %fd693, %fd252, %fd695;
	fma.rn.f64 	%fd887, %fd693, %fd254, %fd697;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r165}, %fd886;
	}
	and.b32  	%r166, %r165, 2145386496;
	setp.lt.u32	%p68, %r166, 1105199104;
	@%p68 bra 	BB0_116;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd886;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd887, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r235, [%rd23];

BB0_116:
	add.s32 	%r45, %r235, 1;
	and.b32  	%r167, %r45, 1;
	shl.b32 	%r168, %r167, 3;
	setp.eq.s32	%p69, %r167, 0;
	selp.f64	%fd699, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p69;
	mul.wide.u32 	%rd108, %r168, 8;
	add.s64 	%rd110, %rd108, %rd27;
	ld.const.f64 	%fd700, [%rd110+8];
	mul.rn.f64 	%fd161, %fd887, %fd887;
	fma.rn.f64 	%fd701, %fd699, %fd161, %fd700;
	ld.const.f64 	%fd702, [%rd110+16];
	fma.rn.f64 	%fd703, %fd701, %fd161, %fd702;
	ld.const.f64 	%fd704, [%rd110+24];
	fma.rn.f64 	%fd705, %fd703, %fd161, %fd704;
	ld.const.f64 	%fd706, [%rd110+32];
	fma.rn.f64 	%fd707, %fd705, %fd161, %fd706;
	ld.const.f64 	%fd708, [%rd110+40];
	fma.rn.f64 	%fd709, %fd707, %fd161, %fd708;
	ld.const.f64 	%fd710, [%rd110+48];
	fma.rn.f64 	%fd162, %fd709, %fd161, %fd710;
	fma.rn.f64 	%fd888, %fd162, %fd887, %fd887;
	@%p69 bra 	BB0_118;

	mov.f64 	%fd838, 0d3FF0000000000000;
	fma.rn.f64 	%fd888, %fd162, %fd161, %fd838;

BB0_118:
	and.b32  	%r169, %r45, 2;
	setp.eq.s32	%p70, %r169, 0;
	@%p70 bra 	BB0_120;

	mov.f64 	%fd712, 0d0000000000000000;
	mov.f64 	%fd713, 0dBFF0000000000000;
	fma.rn.f64 	%fd888, %fd888, %fd713, %fd712;

BB0_120:
	mul.f64 	%fd890, %fd151, %fd888;
	bra.uni 	BB0_121;

BB0_102:
	add.f64 	%fd533, %fd146, 0dC0033D152E971B40;
	add.f64 	%fd534, %fd533, 0d3CA0F539D7DA258E;
	mov.f64 	%fd535, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd536, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd537, %fd536, %fd534, %fd535;
	mov.f64 	%fd538, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd539, %fd537, %fd534, %fd538;
	mov.f64 	%fd540, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd541, %fd539, %fd534, %fd540;
	mov.f64 	%fd542, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd543, %fd541, %fd534, %fd542;
	mov.f64 	%fd544, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd545, %fd543, %fd534, %fd544;
	mov.f64 	%fd546, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd547, %fd545, %fd534, %fd546;
	mov.f64 	%fd548, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd549, %fd547, %fd534, %fd548;
	mov.f64 	%fd550, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd551, %fd549, %fd534, %fd550;
	mov.f64 	%fd552, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd553, %fd551, %fd534, %fd552;
	mov.f64 	%fd554, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd555, %fd553, %fd534, %fd554;
	mov.f64 	%fd556, 0d3F315382BA096A62;
	fma.rn.f64 	%fd557, %fd555, %fd534, %fd556;
	mov.f64 	%fd558, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd559, %fd557, %fd534, %fd558;
	mov.f64 	%fd560, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd561, %fd559, %fd534, %fd560;
	mov.f64 	%fd562, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd563, %fd561, %fd534, %fd562;
	mov.f64 	%fd564, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd565, %fd563, %fd534, %fd564;
	mov.f64 	%fd566, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd567, %fd565, %fd534, %fd566;
	mul.f64 	%fd890, %fd534, %fd567;
	bra.uni 	BB0_121;

BB0_104:
	add.f64 	%fd568, %fd146, 0dC016148F5B2C2E45;
	add.f64 	%fd569, %fd568, 0dBC975054CD60A517;
	mov.f64 	%fd570, 0d3CF83FD1F333EB61;
	mov.f64 	%fd571, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd572, %fd571, %fd569, %fd570;
	mov.f64 	%fd573, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd574, %fd572, %fd569, %fd573;
	mov.f64 	%fd575, 0dBD7846076D004627;
	fma.rn.f64 	%fd576, %fd574, %fd569, %fd575;
	mov.f64 	%fd577, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd578, %fd576, %fd569, %fd577;
	mov.f64 	%fd579, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd580, %fd578, %fd569, %fd579;
	mov.f64 	%fd581, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd582, %fd580, %fd569, %fd581;
	mov.f64 	%fd583, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd584, %fd582, %fd569, %fd583;
	mov.f64 	%fd585, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd586, %fd584, %fd569, %fd585;
	mov.f64 	%fd587, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd588, %fd586, %fd569, %fd587;
	mov.f64 	%fd589, 0dBF0863F481A37337;
	fma.rn.f64 	%fd590, %fd588, %fd569, %fd589;
	mov.f64 	%fd591, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd592, %fd590, %fd569, %fd591;
	mov.f64 	%fd593, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd594, %fd592, %fd569, %fd593;
	mov.f64 	%fd595, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd596, %fd594, %fd569, %fd595;
	mov.f64 	%fd597, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd598, %fd596, %fd569, %fd597;
	mov.f64 	%fd599, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd600, %fd598, %fd569, %fd599;
	mov.f64 	%fd601, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd602, %fd600, %fd569, %fd601;
	mul.f64 	%fd890, %fd569, %fd602;
	bra.uni 	BB0_121;

BB0_106:
	add.f64 	%fd603, %fd146, 0dC0214EB56CCCDECA;
	add.f64 	%fd604, %fd603, 0d3CB51970714C7C25;
	mov.f64 	%fd605, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd606, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd607, %fd606, %fd604, %fd605;
	mov.f64 	%fd608, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd609, %fd607, %fd604, %fd608;
	mov.f64 	%fd610, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd611, %fd609, %fd604, %fd610;
	mov.f64 	%fd612, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd613, %fd611, %fd604, %fd612;
	mov.f64 	%fd614, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd615, %fd613, %fd604, %fd614;
	mov.f64 	%fd616, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd617, %fd615, %fd604, %fd616;
	mov.f64 	%fd618, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd619, %fd617, %fd604, %fd618;
	mov.f64 	%fd620, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd621, %fd619, %fd604, %fd620;
	mov.f64 	%fd622, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd623, %fd621, %fd604, %fd622;
	mov.f64 	%fd624, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd625, %fd623, %fd604, %fd624;
	mov.f64 	%fd626, 0d3F1D79605276949A;
	fma.rn.f64 	%fd627, %fd625, %fd604, %fd626;
	mov.f64 	%fd628, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd629, %fd627, %fd604, %fd628;
	mov.f64 	%fd630, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd631, %fd629, %fd604, %fd630;
	mov.f64 	%fd632, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd633, %fd631, %fd604, %fd632;
	mov.f64 	%fd634, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd635, %fd633, %fd604, %fd634;
	mov.f64 	%fd636, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd637, %fd635, %fd604, %fd636;
	mul.f64 	%fd890, %fd604, %fd637;

BB0_121:
	mov.f64 	%fd891, %fd899;
	@%p49 bra 	BB0_124;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r170, %temp}, %fd899;
	}
	setp.ne.s32	%p72, %r170, 0;
	mov.f64 	%fd891, %fd899;
	@%p72 bra 	BB0_124;

	mov.f64 	%fd714, 0d0000000000000000;
	mul.rn.f64 	%fd891, %fd899, %fd714;

BB0_124:
	mul.f64 	%fd715, %fd891, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r236, %fd715;
	st.local.u32 	[%rd23], %r236;
	cvt.rn.f64.s32	%fd716, %r236;
	neg.f64 	%fd717, %fd716;
	fma.rn.f64 	%fd719, %fd717, %fd250, %fd891;
	fma.rn.f64 	%fd721, %fd717, %fd252, %fd719;
	fma.rn.f64 	%fd892, %fd717, %fd254, %fd721;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r171}, %fd891;
	}
	and.b32  	%r172, %r171, 2145386496;
	setp.lt.u32	%p73, %r172, 1105199104;
	@%p73 bra 	BB0_126;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd891;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd892, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r236, [%rd23];

BB0_126:
	add.s32 	%r49, %r236, 1;
	and.b32  	%r173, %r49, 1;
	shl.b32 	%r174, %r173, 3;
	setp.eq.s32	%p74, %r173, 0;
	selp.f64	%fd723, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p74;
	mul.wide.u32 	%rd115, %r174, 8;
	add.s64 	%rd117, %rd115, %rd27;
	ld.const.f64 	%fd724, [%rd117+8];
	mul.rn.f64 	%fd175, %fd892, %fd892;
	fma.rn.f64 	%fd725, %fd723, %fd175, %fd724;
	ld.const.f64 	%fd726, [%rd117+16];
	fma.rn.f64 	%fd727, %fd725, %fd175, %fd726;
	ld.const.f64 	%fd728, [%rd117+24];
	fma.rn.f64 	%fd729, %fd727, %fd175, %fd728;
	ld.const.f64 	%fd730, [%rd117+32];
	fma.rn.f64 	%fd731, %fd729, %fd175, %fd730;
	ld.const.f64 	%fd732, [%rd117+40];
	fma.rn.f64 	%fd733, %fd731, %fd175, %fd732;
	ld.const.f64 	%fd734, [%rd117+48];
	fma.rn.f64 	%fd176, %fd733, %fd175, %fd734;
	fma.rn.f64 	%fd893, %fd176, %fd892, %fd892;
	@%p74 bra 	BB0_128;

	mov.f64 	%fd735, 0d3FF0000000000000;
	fma.rn.f64 	%fd893, %fd176, %fd175, %fd735;

BB0_128:
	and.b32  	%r175, %r49, 2;
	setp.eq.s32	%p75, %r175, 0;
	@%p75 bra 	BB0_130;

	mov.f64 	%fd736, 0d0000000000000000;
	mov.f64 	%fd737, 0dBFF0000000000000;
	fma.rn.f64 	%fd893, %fd893, %fd737, %fd736;

BB0_130:
	sqrt.rn.f64 	%fd182, %fd893;
	mov.f64 	%fd895, %fd899;
	@%p49 bra 	BB0_133;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r176, %temp}, %fd899;
	}
	setp.ne.s32	%p77, %r176, 0;
	mov.f64 	%fd895, %fd899;
	@%p77 bra 	BB0_133;

	mov.f64 	%fd738, 0d0000000000000000;
	mul.rn.f64 	%fd895, %fd899, %fd738;

BB0_133:
	mul.f64 	%fd739, %fd895, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r237, %fd739;
	st.local.u32 	[%rd23], %r237;
	cvt.rn.f64.s32	%fd740, %r237;
	neg.f64 	%fd741, %fd740;
	fma.rn.f64 	%fd743, %fd741, %fd250, %fd895;
	fma.rn.f64 	%fd745, %fd741, %fd252, %fd743;
	fma.rn.f64 	%fd896, %fd741, %fd254, %fd745;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r177}, %fd895;
	}
	and.b32  	%r178, %r177, 2145386496;
	setp.lt.u32	%p78, %r178, 1105199104;
	@%p78 bra 	BB0_135;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd895;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd896, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r237, [%rd23];

BB0_135:
	add.s32 	%r53, %r237, 1;
	and.b32  	%r179, %r53, 1;
	shl.b32 	%r180, %r179, 3;
	setp.eq.s32	%p79, %r179, 0;
	selp.f64	%fd747, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p79;
	mul.wide.u32 	%rd122, %r180, 8;
	add.s64 	%rd124, %rd122, %rd27;
	ld.const.f64 	%fd748, [%rd124+8];
	mul.rn.f64 	%fd188, %fd896, %fd896;
	fma.rn.f64 	%fd749, %fd747, %fd188, %fd748;
	ld.const.f64 	%fd750, [%rd124+16];
	fma.rn.f64 	%fd751, %fd749, %fd188, %fd750;
	ld.const.f64 	%fd752, [%rd124+24];
	fma.rn.f64 	%fd753, %fd751, %fd188, %fd752;
	ld.const.f64 	%fd754, [%rd124+32];
	fma.rn.f64 	%fd755, %fd753, %fd188, %fd754;
	ld.const.f64 	%fd756, [%rd124+40];
	fma.rn.f64 	%fd757, %fd755, %fd188, %fd756;
	ld.const.f64 	%fd758, [%rd124+48];
	fma.rn.f64 	%fd189, %fd757, %fd188, %fd758;
	fma.rn.f64 	%fd897, %fd189, %fd896, %fd896;
	@%p79 bra 	BB0_137;

	mov.f64 	%fd759, 0d3FF0000000000000;
	fma.rn.f64 	%fd897, %fd189, %fd188, %fd759;

BB0_137:
	and.b32  	%r181, %r53, 2;
	setp.eq.s32	%p80, %r181, 0;
	@%p80 bra 	BB0_139;

	mov.f64 	%fd760, 0d0000000000000000;
	mov.f64 	%fd761, 0dBFF0000000000000;
	fma.rn.f64 	%fd897, %fd897, %fd761, %fd760;

BB0_139:
	add.f64 	%fd762, %fd897, 0d3FF0000000000000;
	mul.f64 	%fd763, %fd67, %fd182;
	mul.f64 	%fd764, %fd763, %fd762;
	mul.f64 	%fd195, %fd890, %fd764;
	@%p49 bra 	BB0_142;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r182, %temp}, %fd899;
	}
	setp.ne.s32	%p82, %r182, 0;
	@%p82 bra 	BB0_142;

	mov.f64 	%fd765, 0d0000000000000000;
	mul.rn.f64 	%fd899, %fd899, %fd765;

BB0_142:
	mul.f64 	%fd766, %fd899, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r238, %fd766;
	st.local.u32 	[%rd23], %r238;
	cvt.rn.f64.s32	%fd767, %r238;
	neg.f64 	%fd768, %fd767;
	fma.rn.f64 	%fd770, %fd768, %fd250, %fd899;
	fma.rn.f64 	%fd772, %fd768, %fd252, %fd770;
	fma.rn.f64 	%fd900, %fd768, %fd254, %fd772;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd899;
	}
	and.b32  	%r184, %r183, 2145386496;
	setp.lt.u32	%p83, %r184, 1105199104;
	@%p83 bra 	BB0_144;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd899;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd900, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r238, [%rd23];

BB0_144:
	and.b32  	%r185, %r238, 1;
	shl.b32 	%r186, %r185, 3;
	setp.eq.s32	%p84, %r185, 0;
	selp.f64	%fd774, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p84;
	mul.wide.u32 	%rd129, %r186, 8;
	add.s64 	%rd131, %rd129, %rd27;
	ld.const.f64 	%fd775, [%rd131+8];
	mul.rn.f64 	%fd201, %fd900, %fd900;
	fma.rn.f64 	%fd776, %fd774, %fd201, %fd775;
	ld.const.f64 	%fd777, [%rd131+16];
	fma.rn.f64 	%fd778, %fd776, %fd201, %fd777;
	ld.const.f64 	%fd779, [%rd131+24];
	fma.rn.f64 	%fd780, %fd778, %fd201, %fd779;
	ld.const.f64 	%fd781, [%rd131+32];
	fma.rn.f64 	%fd782, %fd780, %fd201, %fd781;
	ld.const.f64 	%fd783, [%rd131+40];
	fma.rn.f64 	%fd784, %fd782, %fd201, %fd783;
	ld.const.f64 	%fd785, [%rd131+48];
	fma.rn.f64 	%fd202, %fd784, %fd201, %fd785;
	fma.rn.f64 	%fd901, %fd202, %fd900, %fd900;
	@%p84 bra 	BB0_146;

	mov.f64 	%fd786, 0d3FF0000000000000;
	fma.rn.f64 	%fd901, %fd202, %fd201, %fd786;

BB0_146:
	and.b32  	%r187, %r238, 2;
	setp.eq.s32	%p85, %r187, 0;
	@%p85 bra 	BB0_148;

	mov.f64 	%fd787, 0d0000000000000000;
	mov.f64 	%fd788, 0dBFF0000000000000;
	fma.rn.f64 	%fd901, %fd901, %fd788, %fd787;

BB0_148:
	mul.f64 	%fd789, %fd195, %fd901;
	mul.f64 	%fd208, %fd66, %fd789;
	add.f64 	%fd907, %fd65, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r188}, %fd907;
	}
	and.b32  	%r57, %r188, 2147483647;
	setp.ne.s32	%p86, %r57, 2146435072;
	mov.f64 	%fd903, %fd907;
	@%p86 bra 	BB0_151;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r189, %temp}, %fd907;
	}
	setp.ne.s32	%p87, %r189, 0;
	mov.f64 	%fd903, %fd907;
	@%p87 bra 	BB0_151;

	mov.f64 	%fd790, 0d0000000000000000;
	mul.rn.f64 	%fd903, %fd907, %fd790;

BB0_151:
	mul.f64 	%fd791, %fd903, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r239, %fd791;
	st.local.u32 	[%rd23], %r239;
	cvt.rn.f64.s32	%fd792, %r239;
	neg.f64 	%fd793, %fd792;
	fma.rn.f64 	%fd795, %fd793, %fd250, %fd903;
	fma.rn.f64 	%fd797, %fd793, %fd252, %fd795;
	fma.rn.f64 	%fd904, %fd793, %fd254, %fd797;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd903;
	}
	and.b32  	%r191, %r190, 2145386496;
	setp.lt.u32	%p88, %r191, 1105199104;
	@%p88 bra 	BB0_153;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd903;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd904, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r239, [%rd23];

BB0_153:
	add.s32 	%r61, %r239, 1;
	and.b32  	%r192, %r61, 1;
	shl.b32 	%r193, %r192, 3;
	setp.eq.s32	%p89, %r192, 0;
	selp.f64	%fd799, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p89;
	mul.wide.u32 	%rd136, %r193, 8;
	add.s64 	%rd138, %rd136, %rd27;
	ld.const.f64 	%fd800, [%rd138+8];
	mul.rn.f64 	%fd215, %fd904, %fd904;
	fma.rn.f64 	%fd801, %fd799, %fd215, %fd800;
	ld.const.f64 	%fd802, [%rd138+16];
	fma.rn.f64 	%fd803, %fd801, %fd215, %fd802;
	ld.const.f64 	%fd804, [%rd138+24];
	fma.rn.f64 	%fd805, %fd803, %fd215, %fd804;
	ld.const.f64 	%fd806, [%rd138+32];
	fma.rn.f64 	%fd807, %fd805, %fd215, %fd806;
	ld.const.f64 	%fd808, [%rd138+40];
	fma.rn.f64 	%fd809, %fd807, %fd215, %fd808;
	ld.const.f64 	%fd810, [%rd138+48];
	fma.rn.f64 	%fd216, %fd809, %fd215, %fd810;
	fma.rn.f64 	%fd905, %fd216, %fd904, %fd904;
	@%p89 bra 	BB0_155;

	mov.f64 	%fd811, 0d3FF0000000000000;
	fma.rn.f64 	%fd905, %fd216, %fd215, %fd811;

BB0_155:
	and.b32  	%r194, %r61, 2;
	setp.eq.s32	%p90, %r194, 0;
	@%p90 bra 	BB0_157;

	mov.f64 	%fd812, 0d0000000000000000;
	mov.f64 	%fd813, 0dBFF0000000000000;
	fma.rn.f64 	%fd905, %fd905, %fd813, %fd812;

BB0_157:
	fma.rn.f64 	%fd860, %fd208, %fd905, %fd860;
	@%p86 bra 	BB0_160;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r195, %temp}, %fd907;
	}
	setp.ne.s32	%p92, %r195, 0;
	@%p92 bra 	BB0_160;

	mov.f64 	%fd814, 0d0000000000000000;
	mul.rn.f64 	%fd907, %fd907, %fd814;

BB0_160:
	mul.f64 	%fd815, %fd907, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r240, %fd815;
	st.local.u32 	[%rd23], %r240;
	cvt.rn.f64.s32	%fd816, %r240;
	neg.f64 	%fd817, %fd816;
	fma.rn.f64 	%fd819, %fd817, %fd250, %fd907;
	fma.rn.f64 	%fd821, %fd817, %fd252, %fd819;
	fma.rn.f64 	%fd908, %fd817, %fd254, %fd821;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd907;
	}
	and.b32  	%r197, %r196, 2145386496;
	setp.lt.u32	%p93, %r197, 1105199104;
	@%p93 bra 	BB0_162;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd907;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd22;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd908, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r240, [%rd23];

BB0_162:
	and.b32  	%r198, %r240, 1;
	shl.b32 	%r199, %r198, 3;
	setp.eq.s32	%p94, %r198, 0;
	selp.f64	%fd823, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p94;
	mul.wide.u32 	%rd143, %r199, 8;
	add.s64 	%rd145, %rd143, %rd27;
	ld.const.f64 	%fd824, [%rd145+8];
	mul.rn.f64 	%fd228, %fd908, %fd908;
	fma.rn.f64 	%fd825, %fd823, %fd228, %fd824;
	ld.const.f64 	%fd826, [%rd145+16];
	fma.rn.f64 	%fd827, %fd825, %fd228, %fd826;
	ld.const.f64 	%fd828, [%rd145+24];
	fma.rn.f64 	%fd829, %fd827, %fd228, %fd828;
	ld.const.f64 	%fd830, [%rd145+32];
	fma.rn.f64 	%fd831, %fd829, %fd228, %fd830;
	ld.const.f64 	%fd832, [%rd145+40];
	fma.rn.f64 	%fd833, %fd831, %fd228, %fd832;
	ld.const.f64 	%fd834, [%rd145+48];
	fma.rn.f64 	%fd229, %fd833, %fd228, %fd834;
	fma.rn.f64 	%fd909, %fd229, %fd908, %fd908;
	@%p94 bra 	BB0_164;

	mov.f64 	%fd835, 0d3FF0000000000000;
	fma.rn.f64 	%fd909, %fd229, %fd228, %fd835;

BB0_164:
	and.b32  	%r200, %r240, 2;
	setp.eq.s32	%p95, %r200, 0;
	@%p95 bra 	BB0_166;

	mov.f64 	%fd836, 0d0000000000000000;
	mov.f64 	%fd837, 0dBFF0000000000000;
	fma.rn.f64 	%fd909, %fd909, %fd837, %fd836;

BB0_166:
	fma.rn.f64 	%fd859, %fd208, %fd909, %fd859;
	add.s32 	%r227, %r227, 1;
	setp.ne.s32	%p96, %r227, 10240;
	@%p96 bra 	BB0_47;

	ld.param.u64 	%rd211, [forcomputeKernel_param_3];
	mov.u32 	%r221, %ntid.x;
	mov.u32 	%r220, %tid.x;
	mov.u32 	%r219, %ctaid.x;
	mad.lo.s32 	%r218, %r221, %r219, %r220;
	cvt.u64.u32	%rd210, %r218;
	ld.param.u32 	%r217, [forcomputeKernel_param_11];
	cvt.s64.s32	%rd209, %r217;
	add.s64 	%rd208, %rd210, %rd209;
	ld.param.u32 	%r216, [forcomputeKernel_param_8];
	ld.param.u64 	%rd207, [forcomputeKernel_param_2];
	ld.param.u32 	%r215, [forcomputeKernel_param_9];
	mov.u32 	%r214, %tid.y;
	mov.u32 	%r213, %ctaid.y;
	mov.u32 	%r212, %ntid.y;
	mad.lo.s32 	%r211, %r212, %r213, %r214;
	cvt.u64.u32	%rd206, %r211;
	add.s64 	%rd205, %rd206, %rd208;
	cvt.s64.s32	%rd146, %r215;
	mul.lo.s64 	%rd150, %rd208, %rd146;
	add.s64 	%rd153, %rd205, %rd150;
	cvta.to.global.u64 	%rd154, %rd207;
	shl.b64 	%rd155, %rd153, 3;
	add.s64 	%rd156, %rd154, %rd155;
	st.global.f64 	[%rd156], %fd860;
	add.s64 	%rd157, %rd146, -1;
	add.s64 	%rd158, %rd157, %rd150;
	sub.s64 	%rd159, %rd158, %rd205;
	shl.b64 	%rd160, %rd159, 3;
	add.s64 	%rd161, %rd154, %rd160;
	st.global.f64 	[%rd161], %fd860;
	mul.lo.s64 	%rd162, %rd205, %rd146;
	sub.s64 	%rd163, %rd157, %rd208;
	add.s64 	%rd164, %rd163, %rd162;
	shl.b64 	%rd165, %rd164, 3;
	add.s64 	%rd166, %rd154, %rd165;
	st.global.f64 	[%rd166], %fd860;
	add.s64 	%rd167, %rd162, %rd208;
	shl.b64 	%rd168, %rd167, 3;
	add.s64 	%rd169, %rd154, %rd168;
	st.global.f64 	[%rd169], %fd860;
	add.s32 	%r209, %r215, -1;
	cvt.s64.s32	%rd170, %r209;
	sub.s64 	%rd171, %rd170, %rd205;
	mul.lo.s64 	%rd172, %rd171, %rd146;
	add.s64 	%rd173, %rd172, %rd208;
	shl.b64 	%rd174, %rd173, 3;
	add.s64 	%rd175, %rd154, %rd174;
	st.global.f64 	[%rd175], %fd860;
	sub.s64 	%rd176, %rd170, %rd208;
	mul.lo.s64 	%rd177, %rd176, %rd146;
	cvt.s64.s32	%rd178, %r216;
	add.s64 	%rd179, %rd178, -1;
	add.s64 	%rd180, %rd179, %rd177;
	sub.s64 	%rd181, %rd180, %rd205;
	shl.b64 	%rd182, %rd181, 3;
	add.s64 	%rd183, %rd154, %rd182;
	st.global.f64 	[%rd183], %fd860;
	sub.s64 	%rd184, %rd179, %rd208;
	add.s64 	%rd185, %rd184, %rd172;
	shl.b64 	%rd186, %rd185, 3;
	add.s64 	%rd187, %rd154, %rd186;
	st.global.f64 	[%rd187], %fd860;
	add.s32 	%r210, %r216, -1;
	cvt.s64.s32	%rd188, %r210;
	sub.s64 	%rd189, %rd188, %rd208;
	mul.lo.s64 	%rd190, %rd189, %rd146;
	add.s64 	%rd191, %rd205, %rd190;
	shl.b64 	%rd192, %rd191, 3;
	add.s64 	%rd193, %rd154, %rd192;
	st.global.f64 	[%rd193], %fd860;
	cvta.to.global.u64 	%rd194, %rd211;
	add.s64 	%rd195, %rd194, %rd155;
	st.global.f64 	[%rd195], %fd859;
	add.s64 	%rd196, %rd194, %rd160;
	st.global.f64 	[%rd196], %fd859;
	add.s64 	%rd197, %rd184, %rd162;
	shl.b64 	%rd198, %rd197, 3;
	add.s64 	%rd199, %rd194, %rd198;
	st.global.f64 	[%rd199], %fd859;
	add.s64 	%rd200, %rd194, %rd168;
	st.global.f64 	[%rd200], %fd859;
	add.s64 	%rd201, %rd194, %rd174;
	st.global.f64 	[%rd201], %fd859;
	add.s64 	%rd202, %rd194, %rd182;
	st.global.f64 	[%rd202], %fd859;
	add.s64 	%rd203, %rd194, %rd186;
	st.global.f64 	[%rd203], %fd859;
	add.s64 	%rd204, %rd194, %rd192;
	st.global.f64 	[%rd204], %fd859;

BB0_168:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd1, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB1_13;

	add.s32 	%r15, %r4, -1024;
	shr.u32 	%r16, %r15, 6;
	mov.u32 	%r17, 15;
	sub.s32 	%r5, %r17, %r16;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r16;
	mov.u32 	%r20, 18;
	min.s32 	%r6, %r20, %r19;
	mov.u64 	%rd94, 0;
	setp.ge.s32	%p2, %r5, %r6;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB1_4;

	bfe.u32 	%r21, %r1, 20, 11;
	add.s32 	%r22, %r21, -1024;
	shr.u32 	%r23, %r22, 6;
	sub.s32 	%r25, %r17, %r23;
	mul.wide.s32 	%rd41, %r25, 8;
	mov.u64 	%rd42, __cudart_i2opi_d;
	add.s64 	%rd89, %rd42, %rd41;
	mov.b64 	 %rd43, %fd4;
	shl.b64 	%rd44, %rd43, 11;
	or.b64  	%rd5, %rd44, -9223372036854775808;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r5;

BB1_3:
	.pragma "nounroll";
	ld.const.u64 	%rd47, [%rd89];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd47;    
	mov.b64         {blo,bhi}, %rd5;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd45, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd91], %rd45;
	add.s32 	%r39, %r39, 1;
	sub.s32 	%r26, %r39, %r5;
	mul.wide.s32 	%rd50, %r26, 8;
	add.s64 	%rd91, %rd1, %rd50;
	add.s64 	%rd93, %rd93, 8;
	add.s64 	%rd89, %rd89, 8;
	setp.lt.s32	%p3, %r39, %r6;
	@%p3 bra 	BB1_3;

BB1_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r9, %r3, 63;
	setp.eq.s32	%p4, %r9, 0;
	@%p4 bra 	BB1_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r9;
	shl.b64 	%rd51, %rd96, %r9;
	shr.u64 	%rd52, %rd95, %r28;
	or.b64  	%rd96, %rd51, %rd52;
	shl.b64 	%rd53, %rd95, %r9;
	ld.local.u64 	%rd54, [%rd1+8];
	shr.u64 	%rd55, %rd54, %r28;
	or.b64  	%rd95, %rd55, %rd53;

BB1_6:
	shr.u64 	%rd56, %rd96, 62;
	cvt.u32.u64	%r29, %rd56;
	shr.u64 	%rd57, %rd95, 62;
	shl.b64 	%rd58, %rd96, 2;
	or.b64  	%rd98, %rd58, %rd57;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd59, %rd96, 61;
	cvt.u32.u64	%r30, %rd59;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.eq.s32	%p5, %r40, 0;
	selp.b32	%r34, %r32, %r33, %p5;
	cvta.to.local.u64 	%rd60, %rd37;
	st.local.u32 	[%rd60], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB1_8;

	mov.u64 	%rd64, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd64;
	mov.b64         {a2,a3}, %rd64;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB1_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB1_10;

	shl.b64 	%rd67, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd68, %rd97, %r36;
	or.b64  	%rd98, %rd68, %rd67;

BB1_10:
	mov.u64 	%rd72, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd72;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd69, {r0,r1};     
	mov.b64         %rd100, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd100, 1;
	@%p8 bra 	BB1_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd69;
	mov.b64         {a2,a3}, %rd100;
	mov.b64         {b0,b1}, %rd69;
	mov.b64         {b2,b3}, %rd100;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd73, {r0,r1};
	mov.b64         %rd100, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB1_12:
	cvt.u64.u32	%rd79, %r40;
	shl.b64 	%rd80, %rd79, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd81, %r38;
	shl.b64 	%rd82, %rd81, 52;
	add.s64 	%rd83, %rd100, 1;
	shr.u64 	%rd84, %rd83, 10;
	add.s64 	%rd85, %rd84, 1;
	shr.u64 	%rd86, %rd85, 1;
	add.s64 	%rd87, %rd86, %rd82;
	or.b64  	%rd88, %rd87, %rd80;
	mov.b64 	 %fd4, %rd88;

BB1_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


